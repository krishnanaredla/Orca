{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(r'C:\\work\\python-packages\\orca\\data\\membership.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uniqueID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- recID: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- dates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "windowPartition = Window.partitionBy(col('uniqueID')).orderBy(col('dates').asc())\n",
    "df.withColumn('rank',row_number().over(windowPartition)).filter(col('rank')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "from typing import List\n",
    "def dedupsByRank(df:DataFrame,\n",
    "                 partitionBy:List[str]=List[None],\n",
    "                 orderBy:List[str]=List[None],\n",
    "                rankType:Column=row_number())-> DataFrame :\n",
    "    windowBy = Window.partitionBy(*list(map(lambda col_name:col(col_name),partitionBy)))\\\n",
    "                     .orderBy(*list(map(lambda col_name:col(col_name),orderBy)))\n",
    "    df = df.withColumn('rank',rankType.over(windowPartition)).filter(col('rank')==1).drop('rank')\n",
    "    return df\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Dict,Union\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def dedupsByRank(\n",
    "    df: DataFrame,\n",
    "    partitionCols: List,\n",
    "    orderCols: Union[List, Dict[str, List]],\n",
    "    rankType:Column = row_number(),\n",
    ") -> DataFrame:\n",
    "    orderCols = (\n",
    "        {\"columns\": list(orderCols), \"asc\": [], \"desc\": []}\n",
    "        if isinstance(orderCols, list)\n",
    "        else orderCols\n",
    "    )\n",
    "    _ = (\n",
    "        lambda col_name: col(col_name).asc()\n",
    "        if col_name in orderCols[\"asc\"]\n",
    "        else (col(col_name).desc() if col_name in orderCols[\"desc\"] else col(col_name))\n",
    "    )\n",
    "    windowBy = Window.partitionBy(\n",
    "        *list(map(lambda col_name: col(col_name), partitionCols))\n",
    "    ).orderBy(*list(map(lambda col_name: _(col_name), orderCols['columns'])))\n",
    "    return (\n",
    "        df.withColumn(\"rank\", rankType.over(windowBy))\n",
    "        .filter(col(\"rank\") == 1)\n",
    "        .drop(\"rank\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ =lambda col_name: col(col_name).asc() if col_name in orderCols['asc'] else (col(col_name).desc() if col_name in orderCols['desc'] else col(col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'dates ASC NULLS FIRST'>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_('dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rankType = row_number()\n",
    "type(rankType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'dates ASC NULLS FIRST'>, Column<b'cost DESC NULLS LAST'>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionBy =  ['uniqueID','x']\n",
    "\n",
    "orderCols = {\n",
    "    'columns' : ['dates','cost'],\n",
    "    'asc' : ['dates'],\n",
    "    'desc' : ['cost']\n",
    "}\n",
    "\n",
    "#orderCols = ['dates','cost']\n",
    "\n",
    "if isinstance(orderCols,list):\n",
    "    orderCols = {'columns':list(orderCols),\n",
    "                    'asc':[],\n",
    "                    'desc':[]}\n",
    "\n",
    "def _(col_name):\n",
    "    if col_name in orderCols['asc']:\n",
    "        return col(col_name).asc()\n",
    "    elif col_name in orderCols['desc']:\n",
    "        return col(col_name).desc()\n",
    "    else:\n",
    "        return col(col_name)\n",
    "list(map(lambda col_name : _(col_name),orderCols['columns']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columns': ['dates', 'cost'], 'asc': ['dates'], 'desc': ['cost']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderCols = {\n",
    "    'columns' : ['dates','cost'],\n",
    "    'asc' : ['dates'],\n",
    "    'desc' : ['cost']\n",
    "}\n",
    "orderCols\n",
    "#dedupsByRank(df,['uniqueID','cost'],orderCols).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uniqueID: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- recID: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- dates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dedupsByRank(df,['uniqueID'],['dates','cost']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2020-02-20T12:00:12.345000000')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1582200012345</td>\n",
       "      <td>2020-02-20 12:00:12.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1582200012346</td>\n",
       "      <td>2020-02-20 12:00:12.346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                    time\n",
       "0  1582200012345 2020-02-20 12:00:12.345\n",
       "1  1582200012346 2020-02-20 12:00:12.346"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
