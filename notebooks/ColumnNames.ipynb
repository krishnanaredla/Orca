{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([{'id': 1, 'value': 1,'amount':2}, {'id': 2, 'value': 2,'amount':3}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from typing import List\n",
    "\n",
    "def addColumnPrefix(df:DataFrame,\n",
    "                    prefix:str='prefix',\n",
    "                    colsList:List=[]):\n",
    "    if not colsList:\n",
    "        colsList = df.columns\n",
    "    def _keyExists(s):\n",
    "        return s in colsList\n",
    "    cols = list(map(\n",
    "             lambda col_name:F.col(col_name).alias('{0}_{1}'.format(prefix,col_name))\n",
    "             if _keyExists(col_name) else F.col(col_name),df.columns))\n",
    "    return df.select(*cols)\n",
    "    #if not colsList:\n",
    "    #    colsList = df.columns\n",
    "    #return df.select([F.col(c).alias('{0}_{1}'.format(prefix,c)) if c in colsList else\n",
    "    #                  F.col(c).alias('{0}'.format(c))\n",
    "    #                  for c in df.columns])\n",
    "\n",
    "def addColumnSuffix(df:DataFrame,\n",
    "                    suffix:str='suffix',colsList:List=[]):\n",
    "    if not colsList:\n",
    "        colsList = df.columns\n",
    "    def _keyExists(s):\n",
    "        return s in colsList\n",
    "    cols = list(map(\n",
    "             lambda col_name:F.col(col_name).alias('{0}_{1}'.format(col_name,suffix))\n",
    "             if _keyExists(col_name) else F.col(col_name),df.columns))\n",
    "    return df.select(*cols)\n",
    "    #return df.select([F.col(c).alias('{0}_{1}'.format(c,suffix)) if c in colsList else\n",
    "    #                  F.col(c).alias('{0}'.format(c))\n",
    "    #                  for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----+\n",
      "|amount|test_id|value|\n",
      "+------+-------+-----+\n",
      "|     2|      1|    1|\n",
      "|     3|      2|    2|\n",
      "+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addColumnPrefix(df,prefix='test',colsList=['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+\n",
      "|test_amount|test_id|test_value|\n",
      "+-----------+-------+----------+\n",
      "|          2|      1|         1|\n",
      "|          3|      2|         2|\n",
      "+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addColumnPrefix(df,prefix='test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(r'C:\\work\\python-packages\\mimic\\mimic\\examples\\orders.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['orderID', 'OrderDate', 'OrderValue', 'OrderType', 'OrderCategory', 'city']\n",
      "+------------+--------------------+---------------+--------------+------------------+---------+\n",
      "|test_orderID|      test_OrderDate|test_OrderValue|test_OrderType|test_OrderCategory|test_city|\n",
      "+------------+--------------------+---------------+--------------+------------------+---------+\n",
      "| ORDLBWA5119|2004-10-02 09:28:...|        8135.52|           COD|              Home|   durham|\n",
      "| ORDJQOW0469|2013-08-30 12:04:...|        6013.05|           COD|              Home|  toronto|\n",
      "| ORDADHV6229|2017-09-13 20:33:...|        5115.79|           COD|           Furnish|  toronto|\n",
      "| ORDOPWV1894|2004-03-03 15:36:...|        5361.52|           COD|       Electronics|  toronto|\n",
      "| ORDENUU3212|2004-05-26 21:57:...|        2505.97|           COD|           Furnish|   durham|\n",
      "| ORDKWLI8989|2013-05-30 07:26:...|        6577.39|           COD|              Home|  halifax|\n",
      "| ORDNTEZ1527|2013-02-18 01:17:...|         5928.3|           COD|           Furnish|  halifax|\n",
      "| ORDLNTY3345|2008-10-03 02:34:...|        6103.64|           COD|           Furnish|   durham|\n",
      "| ORDXYPI3010|2008-09-09 08:58:...|        5628.92|           COD|           Furnish|  halifax|\n",
      "| ORDSPVJ3801|2001-03-05 21:37:...|        4410.64|           COD|              Home|  halifax|\n",
      "| ORDZYPX5707|2002-08-27 09:01:...|         2024.7|           COD|       Electronics|  halifax|\n",
      "| ORDPSVH1562|2006-10-08 23:29:...|        9683.48|           COD|              Home|  toronto|\n",
      "| ORDDNHV8456|2015-12-30 03:30:...|        4724.65|           COD|              Home|   durham|\n",
      "| ORDQQFP4942|2000-03-09 18:39:...|        2060.94|           COD|              Home|  halifax|\n",
      "| ORDKZUD7275|2000-06-03 01:05:...|        5607.57|           COD|           Furnish|  halifax|\n",
      "| ORDEBLV9972|2015-09-25 07:44:...|        3131.81|           COD|              Home|  toronto|\n",
      "| ORDPLMB0588|2005-10-05 23:01:...|         5073.8|           COD|              Home|   durham|\n",
      "| ORDIGAC5444|2019-01-12 12:12:...|        4665.17|           COD|       Electronics|  halifax|\n",
      "| ORDUIJV8919|2010-04-22 02:57:...|         9942.8|           COD|       Electronics|  halifax|\n",
      "| ORDYFHV5031|2020-06-09 00:50:...|        2979.93|           COD|       Electronics|   durham|\n",
      "+------------+--------------------+---------------+--------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addColumnPrefix(data,prefix='test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount', 'id', 'value']\n",
      "+-----------+---+----------+\n",
      "|test_amount| id|test_value|\n",
      "+-----------+---+----------+\n",
      "|          2|  1|         1|\n",
      "|          3|  2|         2|\n",
      "+-----------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addColumnPrefix(df,prefix='test',colsList=['id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'id', 'value']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsList = ['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x for x in df.columns if x not in colsList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'value']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def removeColumnSpaces(df:DataFrame)->DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a suffix to Column names\n",
    "    Args:\n",
    "        df       : pyspark DataFrame\n",
    "    Return:\n",
    "        pyspark Dataframe\n",
    "    Example:\n",
    "    >>>df = spark.createDataFrame([{'id': 1, 'goods value': 1,'total amount':2}])\n",
    "    >>>removeColumnSpaces(df).show()\n",
    "        +----------+---+-----------+\n",
    "        |goodsvalue| id|totalamount|\n",
    "        +----------+---+-----------+\n",
    "        |         1|  1|          2|\n",
    "        +----------+---+-----------+\n",
    "    \n",
    "    \"\"\"\n",
    "    return df.select([F.col(c).alias('{0}'.format(re.sub(r\"\\s+\", \"\", c, flags=re.UNICODE))) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = spark.createDataFrame([{'id': 1, 'goods value': 1,'total amount':2}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+\n",
      "|goods value| id|total amount|\n",
      "+-----------+---+------------+\n",
      "|          1|  1|           2|\n",
      "+-----------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+\n",
      "|goodsvalue| id|totalamount|\n",
      "+----------+---+-----------+\n",
      "|         1|  1|          2|\n",
      "+----------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "removeColumnSpaces(cd).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'amount':'cash','id':'uniqueID','value':'transaction'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cash', 'uniqueID', 'transaction']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(columnsMapping.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cash'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsMapping['amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount', 'id', 'value']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnKeys(s):\n",
    "    return mapping[s]\n",
    "def ifKeyExists(s):\n",
    "    return s in mapping\n",
    "cols =  list(map(\n",
    "         lambda col_name:F.col(col_name).alias(returnKeys(col_name))\n",
    "                 if ifKeyExists(col_name) else F.col(col_name),df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<b'amount AS `cash`'>,\n",
       " Column<b'id AS `uniqueID`'>,\n",
       " Column<b'value AS `transaction`'>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict,Callable\n",
    "\n",
    "def withSomeColumnsRenamed(df:DataFrame,\n",
    "                              mapping:Dict)->DataFrame:\n",
    "    def _keys(s):\n",
    "        return mapping[s]\n",
    "    def _keysExists(s):\n",
    "        return s in mapping\n",
    "    cols = list(map(\n",
    "         lambda col_name:F.col(col_name).alias(_keys(col_name))\n",
    "                 if _keysExists(col_name) else F.col(col_name),df.columns))\n",
    "    return df.select(*cols)\n",
    "\n",
    "def withColumnsRenamedFunc(df:DataFrame,func:Callable)->DataFrame:\n",
    "    cols = list(map(\n",
    "            lambda col_name:F.col(col_name).alias(func(col_name)),df.columns))\n",
    "    return df.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+\n",
      "|cash|uniqueID|transaction|\n",
      "+----+--------+-----------+\n",
      "|   2|       1|          1|\n",
      "|   3|       2|          2|\n",
      "+----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "withSomeColumnsRenamed(df,mapping).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+\n",
      "|cash| id|value|\n",
      "+----+---+-----+\n",
      "|   2|  1|    1|\n",
      "|   3|  2|    2|\n",
      "+----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def renameF(s):\n",
    "    if 'amount' in s:\n",
    "        return 'cash'\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "withColumnsRenamedFunc(df,renameF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df = spark.createDataFrame([{'id': 1, 'value': 1,'amount':'Some Value','currentTime':datetime.datetime.now()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: string (nullable = true)\n",
      " |-- currentTime: timestamp (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df._jdf.schema().treeString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(amount,StringType,true),StructField(id,LongType,true),StructField(value,LongType,true)))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "type(StringType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringType, LongType, LongType]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x:x.dataType,df.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(None.__ne__, list(map(lambda x:x.name if isinstance(x.dataType,StringType) else None,df.schema))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from typing import Any,Type\n",
    "import datetime \n",
    "import decimal\n",
    "\n",
    "_type_mappings =  {type(None): NullType,\n",
    " bool: BooleanType,\n",
    " int: LongType,\n",
    " float: DoubleType,\n",
    " str: StringType,\n",
    " bytearray: BinaryType,\n",
    " decimal.Decimal: DecimalType,\n",
    " datetime.date: DateType,\n",
    " datetime.datetime: TimestampType,\n",
    " datetime.time: TimestampType,\n",
    " bytes: BinaryType}\n",
    "\n",
    "def selectByDtype(df:DataFrame,dtypes:Any)->DataFrame:\n",
    "    if isinstance(dtypes,list):\n",
    "        for val in range(len(dtypes)):\n",
    "            if dtypes[val] in _type_mappings.keys():\n",
    "                dtypes[val] = _type_mappings[dtypes[val]]\n",
    "        dtypes = tuple(dtypes)\n",
    "    elif dtypes in _type_mappings.keys():\n",
    "        dtypes = _type_mappings[dtypes]\n",
    "    cols=list(filter(None.__ne__, \n",
    "                     list(map(lambda field:field.name if isinstance(field.dataType,dtypes) else None,df.schema))))\n",
    "    return df.select(*cols)\n",
    "\n",
    "def getColsByDtype(df:DataFrame,dtypes:Any)->List:\n",
    "    if isinstance(dtypes,list):\n",
    "        for val in range(len(dtypes)):\n",
    "            if dtypes[val] in _type_mappings.keys():\n",
    "                dtypes[val] = _type_mappings[dtypes[val]]\n",
    "        dtypes = tuple(dtypes)\n",
    "    elif dtypes in _type_mappings.keys():\n",
    "        dtypes = _type_mappings[dtypes]\n",
    "    cols=list(filter(None.__ne__, \n",
    "                     list(map(lambda field:field.name if isinstance(field.dataType,dtypes) else None,df.schema))))\n",
    "    return cols\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = [datetime.datetime,LongType]\n",
    "for val in range(len(dtype)):\n",
    "    if dtype[val] in _type_mappings.keys():\n",
    "        dtype[val] = _type_mappings[dtype[val]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyspark.sql.types.TimestampType, pyspark.sql.types.LongType]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|         currentTime| id|value|\n",
      "+--------------------+---+-----+\n",
      "|2020-10-29 01:00:...|  1|    1|\n",
      "+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectByDtype(df,[datetime.datetime,int]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [TimestampType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentTime\n"
     ]
    }
   ],
   "source": [
    "for field in df.schema:\n",
    "    if isinstance(field.dataType,tuple(dtypes)):\n",
    "        print(field.name)\n",
    "    #else:\n",
    "        #print(field.dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L',\n",
       " 'o',\n",
       " 'n',\n",
       " 'g',\n",
       " 'T',\n",
       " 'y',\n",
       " 'p',\n",
       " 'e',\n",
       " ',',\n",
       " 'S',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " 'T',\n",
       " 'y',\n",
       " 'p',\n",
       " 'e']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x='LongType,StringType'\n",
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{NoneType: pyspark.sql.types.NullType,\n",
       " bool: pyspark.sql.types.BooleanType,\n",
       " int: pyspark.sql.types.LongType,\n",
       " float: pyspark.sql.types.DoubleType,\n",
       " str: pyspark.sql.types.StringType,\n",
       " bytearray: pyspark.sql.types.BinaryType,\n",
       " decimal.Decimal: pyspark.sql.types.DecimalType,\n",
       " datetime.date: pyspark.sql.types.DateType,\n",
       " datetime.datetime: pyspark.sql.types.TimestampType,\n",
       " datetime.time: pyspark.sql.types.TimestampType,\n",
       " bytes: pyspark.sql.types.BinaryType}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "T._type_mappings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
